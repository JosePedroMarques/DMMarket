% LaTeX file for a 1 page document
\documentclass[12pt]{article}
\usepackage[english]{babel} 
\usepackage[utf8]{inputenc}

\title{Error detection in transaction's data using knowledge extraction and machine learning}

\author{
	José Pedro Marques and Tiago Pereira\thanks{Thanks to the professors Carlos Soares and João Mendes Moreira}\\
	\small Master in Informatics and Computing Engineering students\\[-0.8ex]
	\small Faculdade de Engenharia Universidade do Porto, PT\\
	\small \texttt{jose.pedro.marques@fe.up.pt, tiago.pereira@fe.up.pt}\\
}

\begin{document}
\maketitle

\begin{abstract}
Data Mining is a very broad area, with several algorithms applicable to the same problem.
The information from each transaction a Portuguese company makes with an EU country reaches the Portuguese Institute of Statistics (INE) through the INTRASTAT form. As in all manual processes, both the form filling and the insertion in the database are error prone, which could lead to incorrect/inconsistent data therefore a specialised staff manually analyses all the transactions with the aim to detect and correct those errors.
The purpose of this paper is to show how although fully automatic detection of the errors is difficult, some work can be done to partly automate this process with outlier detection algorithms.
\end{abstract}

\section{Introduction}

The purpose of this article is to present how various algorithms can be used to help a specialised staff to correct errors in the filling of the INTRASTAT form, with optimum results.
This work can be described has a pre data classification.

This article will include a description of the problem (Section \ref{subsec:description}) , the data available (Section \ref{subsec:data}) and it's difficulties (Section \ref{subsec:difficulties}). This is followed by a description of the chosen algorithms (Section \ref{subsec:algorithms}).

\section{The Problem}
\subsection{Description}
\label{subsec:description}
The information from each transaction a Portuguese company makes with an EU country reaches the Portuguese Institute of Statistics(INE) through the INTRASTAT form. In this form, the company provides information  about the transaction type (import/export), the item id, the weight of goods traded, the total cost, etc. Then this data is manually inserted into a database at INE. Figure 1 presents an excerpt of the data.

As in all manual processes, both the form filling and the insertion in the database are error prone, which could lead to incorrect/inconsistent data. For instance, an incorrectly introduced item id will associate a transaction with the wrong item. While some errors may be irrelevant to some statistics, some errors can influence them greatly.

Therefore, when all of the transactions relative to a month have been entered into the database, they are manually verified with the aim of detecting and correcting as many errors as possible. In this search, the experts try to detect unusual values in the values that describe the transactions.
Given that the number of transactions declared monthly is in the order of tens of thousands, this is a very costly process.

The idea then is to automatically select a subset of the transactions that includes almost all the errors that the experts would detect by looking at all the transactions.

\subsection{Data}
\label{subsec:data}
The data is organized in 2 files per month, one with the outgoing transactions and one with the incoming. Each line is a transaction, with 18 attributes, such as item id, price paid, weight, etc. The last attribute is a boolean attribute error, which indicates if that transaction contains (or not) an error.
Since our problem is outlier detection in transaction from Portugal to the EU, an error in one item may not be an error on another, therefore we felt the need separate and compile the information by item, so each item can be analysed separately.

\subsection{Difficulties}
\label{subsec:difficulties}

One of the problems with the data is related to the attribute "weight", as it might contain null values when the weight of an item as less than a given threshold. But the biggest difficulty we will face is the obviously unbalanced classes, as we expect only a few errors in the totality of the transactions. Therefore, the algorithms must be carefully chosen, having this problem in mind.

\subsection{Chosen Algorithms}
\label{subsec:algorithms}
\subsubsection{Clustering for Outlier Detection}
Some of existing work in outlier detection lies in the field of statistics. Intuitively, outlier can be defined as given by Hawkins (1980).
Every value assigned to a small cluster containing significantly fewer points than the others, enough to arouse suspicion that it is generated by a different mechanism., is considered an outlier (Hawkins-Outlier).

\subsubsection{Decision Trees for Outlier Detection}
Algorithms based on decision trees, learn from a set of pre-classified examples, and build a model of the regularities found used to classify new cases.
This kind of technique is better suited for the detection of outliers, where an isolated branch is classified has an outlier.
This is an algorithm that as the advantage that it doesn't require the need to define the number of clusters, and it is not scalable, this is, he's complexity over time is his numbers of objects (O($n^2$)). Although the interpretation of results can be subjective.

\subsubsection{Cell based outlier detection}
This algorithm can be described has a fast outlier detection algorithm for large datasets. This algorithm can avoid large computations on the majority part of dataset by filtering the initial dataset.
A large dataset is loaded into memory by blocks, and the data are placed into appropriate cells based on their values. Each cell holds a certain number of data, which represents the cell's density. Data located in high density cells who have no nearness relationship with local outlier factor calculation are filtered. And we record these cells density for the next block of data filled in. The final calculation will be done on those data in low density cells. This way, we can handle a large dataset which can't be loaded into memory all at once, improving the algorithm's efficiency by reducing many useless computations. The time complexity is O(N).

\subsection{Experimental Setup}
\label{subsec:experimentalSetup}
The first step was to separate and combine the data in a way that we could do a proper analyses, so for that reason it was created a program that read from a directory data from different sources and it would combine the containing information by item and saved it in a new file.

\begin{thebibliography}{9}

\bibitem{itc:2007}
Carlos Soares and João Mendes Moreira; Knowledge Extraction and Machine Learning; Course’s handouts 2012.

\bibitem{itc:2007}
Soares, Costa, Cortez, Cavalho; Error Detection in Foreign Trade Data   using Statistical and Machine Learning Algorithms.

\end{thebibliography}

\end{document}
